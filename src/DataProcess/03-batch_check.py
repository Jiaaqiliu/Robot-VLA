import time
import json
import os
from openai import OpenAI

OPENAI_API_KEY = "sk-proj-**"
client = OpenAI(api_key=OPENAI_API_KEY)

# è¾“å‡ºç›®å½•é…ç½®
file_name = "planning_with_context_task_video"
OUTPUT_DIR = f"/home/jqliu/Myprojects/RoboBrain/ShareRobot/planning/Output/{file_name}_gpt_process_output"
POLL_INTERVAL = 10  # æ¯éš”å¤šå°‘ç§’è½®è¯¢ä¸€æ¬¡
SUBMITTED_BATCHES_FILE = f"/home/jqliu/Myprojects/RoboBrain/ShareRobot/planning/Output/{file_name}_batches/submitted_batches.json"
BATCH_STATUS_LOG_FILE = os.path.join(OUTPUT_DIR, "batch_status_log.txt")
FINAL_MERGED_JSON = os.path.join(OUTPUT_DIR, "final_merged_output.json")

def normalize_batch_ids_file(file_path):
    """
    è§„èŒƒåŒ–batch IDæ–‡ä»¶çš„æ ¼å¼ï¼Œå°†å…¶è½¬æ¢ä¸ºæ ‡å‡†JSONæ ¼å¼
    """
    try:
        # è¯»å–åŸå§‹æ–‡ä»¶å†…å®¹
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read().strip()
        
        # å°†å†…å®¹æŒ‰è¡Œåˆ†å‰²å¹¶æ¸…ç†
        batch_ids = [line.strip() for line in content.split('\n') if line.strip()]
        
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶è·¯å¾„
        temp_file = file_path + '.temp'
        
        # å°†è§„èŒƒåŒ–çš„JSONå†™å…¥ä¸´æ—¶æ–‡ä»¶
        with open(temp_file, 'w', encoding='utf-8') as f:
            json.dump(batch_ids, f, indent=2)
        
        # æ›¿æ¢åŸæ–‡ä»¶
        os.replace(temp_file, file_path)
        print(f"âœ… Normalized batch IDs file format: {file_path}")
        return batch_ids
        
    except Exception as e:
        print(f"âš ï¸ Error normalizing batch IDs file: {e}")
        return None

def load_batch_ids(file_path):
    """
    åŠ è½½å¹¶éªŒè¯batch IDæ–‡ä»¶
    """
    try:
        # é¦–å…ˆå°è¯•ç›´æ¥ä½œä¸ºJSONè¯»å–
        with open(file_path, 'r', encoding='utf-8') as f:
            try:
                batch_ids = json.load(f)
                if isinstance(batch_ids, list):
                    return batch_ids
            except json.JSONDecodeError:
                print("ğŸ“ Batch IDs file is not in JSON format, attempting to normalize...")
                return normalize_batch_ids_file(file_path)
    except Exception as e:
        print(f"âŒ Error loading batch IDs file: {e}")
        return None

def ensure_output_dir():
    """ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨"""
    if not os.path.exists(OUTPUT_DIR):
        os.makedirs(OUTPUT_DIR)
        print(f"ğŸ“ Created output directory: {OUTPUT_DIR}")

def wait_for_batch(batch_id):
    print(f"â³ Waiting for batch {batch_id} to complete...")
    while True:
        batch = client.batches.retrieve(batch_id)
        status = batch.status
        print(f"ğŸ“¦ Status: {status} | Completed: {batch.request_counts.completed} / {batch.request_counts.total}")
        if status == "completed":
            print("âœ… Batch completed!")
            return batch.output_file_id
        elif status == "failed":
            print("âŒ Batch failed.")
            if batch.error_file_id:
                print("Error file ID:", batch.error_file_id)
            return None
        elif status == "expired":
            print("âš ï¸ Batch expired before completion.")
            return None
        time.sleep(POLL_INTERVAL)

def download_output_file(file_id, output_path):
    print(f"â¬‡ï¸ Downloading output file {file_id}...")
    response = client.files.content(file_id)
    content = response.text
    with open(output_path, "w", encoding="utf-8") as f:
        f.write(content)
    print(f"ğŸ“ Saved output to {output_path}")

def merge_jsonl_to_json(jsonl_path, output_json):
    print(f"ğŸ”„ Merging {jsonl_path} into structured JSON...")
    results = []
    with open(jsonl_path, "r", encoding="utf-8") as f:
        for line in f:
            try:
                obj = json.loads(line)
                custom_id = obj.get("custom_id")
                reasoning = obj.get("response", {}).get("body", {}).get("choices", [{}])[0].get("message", {}).get("content", "")
                results.append({"custom_id": custom_id, "reasoning": reasoning})
            except Exception as e:
                print(f"âš ï¸ Error parsing line: {e}")
    with open(output_json, "w", encoding="utf-8") as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    print(f"âœ… Merged results saved to {output_json}")
    return results

def merge_all_json_files(batch_ids):
    """
    æŒ‰ç…§åŸå§‹batch_idsçš„é¡ºåºåˆå¹¶æ‰€æœ‰JSONæ–‡ä»¶
    """
    print("ğŸ”„ Starting final merge of all JSON files...")
    all_results = []
    
    for batch_id in batch_ids:
        json_file = os.path.join(OUTPUT_DIR, f"merged_{batch_id}.json")
        if os.path.exists(json_file):
            try:
                with open(json_file, 'r', encoding='utf-8') as f:
                    batch_results = json.load(f)
                    all_results.extend(batch_results)
                print(f"âœ… Successfully merged {json_file}")
            except Exception as e:
                print(f"âš ï¸ Error merging {json_file}: {e}")
        else:
            print(f"âš ï¸ Warning: {json_file} not found")
    
    # ä¿å­˜æœ€ç»ˆåˆå¹¶çš„ç»“æœ
    with open(FINAL_MERGED_JSON, 'w', encoding='utf-8') as f:
        json.dump(all_results, f, indent=2, ensure_ascii=False)
    print(f"ğŸ‰ Final merged file saved as {FINAL_MERGED_JSON}")
    print(f"ğŸ“Š Total entries: {len(all_results)}")
    return all_results

def track_and_download_batches():
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    ensure_output_dir()
    
    if not os.path.exists(SUBMITTED_BATCHES_FILE):
        print(f"âš ï¸ {SUBMITTED_BATCHES_FILE} does not exist.")
        return
    
    # åŠ è½½å¹¶è§„èŒƒåŒ–batch IDs
    batch_ids = load_batch_ids(SUBMITTED_BATCHES_FILE)
    if not batch_ids:
        print("âŒ Failed to load batch IDs. Exiting...")
        return
    
    print(f"ğŸ“‹ Loaded {len(batch_ids)} batch IDs")
    
    with open(BATCH_STATUS_LOG_FILE, "w", encoding="utf-8") as log_file:
        for batch_id in batch_ids:
            batch = client.batches.retrieve(batch_id)
            status = batch.status
            log_file.write(f"Batch ID: {batch_id} | Status: {status}\n")
            print(f"Batch ID: {batch_id} | Status: {status}")
            time.sleep(POLL_INTERVAL)

            if status == "completed":
                output_file_id = batch.output_file_id
                jsonl_path = os.path.join(OUTPUT_DIR, f"batch_{batch_id}.jsonl")
                json_path = os.path.join(OUTPUT_DIR, f"merged_{batch_id}.json")
                download_output_file(output_file_id, jsonl_path)
                merge_jsonl_to_json(jsonl_path, json_path)
    
    # åœ¨æ‰€æœ‰æ‰¹æ¬¡å¤„ç†å®Œæˆåï¼Œæ‰§è¡Œæœ€ç»ˆåˆå¹¶
    if batch_ids:
        print("\nğŸ”„ All batches processed, starting final merge...")
        merge_all_json_files(batch_ids)
    else:
        print("\nâš ï¸ No completed batches to merge")

if __name__ == "__main__":
    track_and_download_batches()